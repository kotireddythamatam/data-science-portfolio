{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1f1b48-c75a-417a-83ae-2f067a608b23",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c875603b-7a7c-4c02-a9d6-e3d3e622e973",
   "metadata": {},
   "source": [
    "Activation Function:\n",
    "    Activation functions are deside whether nuron should fire or not and it adds non-linearity to the network.\n",
    "    Fire means activation function produces strong non-zero output.\n",
    "    Doesn't fire means activation output is zero and next layer gets no signal from this neuron \n",
    "\n",
    "Sigmoid(Logistic):\n",
    "    f(x) = 1 / (1 + e ** -x)\n",
    "    Output between 0 and 1, best for binary propabilities, smooth, continuous and differentiable.\n",
    "    Occurs vanishing gradients problem, non zero centered cause inefficient weight updates, computationally more expensive due to non zero centered        and exponentials.\n",
    "\n",
    "Tanh(Hyperbolic Tangent): Maps input into (-1, 1)\n",
    "    f(x) = (e**x - e**-x) / (e**x + e**-x)\n",
    "    Zero centered cause better convergance than sigmoid, smooth and differentiable.\n",
    "    Sometimes suffers vanishing gradients like sigmoid, computationally expensive due to exponentials.\n",
    "\n",
    "ReLu(Rectified Linear Unit): Pass positive values as-is, but make negative values to 0\n",
    "    f(x) = max(0, x)\n",
    "    Helps in reducing vanishing gradients, always faster convergance, simple and computationally efficient\n",
    "    Nueron can die if many inputs are negative, Not zero centered\n",
    "\n",
    "Leaky ReLU: same as Relu but keep small negative values to keep learning\n",
    "    f(x) = max(ax, x) where a = 0.001\n",
    "    Fixes ReLu nueron dead problem, speed and efficient.\n",
    "    Non centered and small slope a must be tuned, no universal value for a, there may be a chance that vanishing gradients problem will occur\n",
    "\n",
    "Exponential Linear Units(ELU):\n",
    "    f(x) = x if x> 0 else (a(e**x -1)\n",
    "    Fixes dead nueron and vanishing gradient problems. Zero centered. Derivative never become zero.\n",
    "    Slightly slow due to exponential and need to choose \"a\".\n",
    "\n",
    "Parametric ReLU (PReLU): same as leaky Relu but \"a\" is learned during training.\n",
    "    f(x) = max(ax, x)\n",
    "    Learns best negative slopes automatically, out performed leaky relu.\n",
    "    Slightly more complex.\n",
    "\n",
    "Softmax:\n",
    "    f(x) = e**xi / sum(e**xj)\n",
    "    Each output between 0 to 1 and sum of all outputs will be equal to 1.\n",
    "    Sensitive to large input values and used for only output layer, not for hidden layers.\n",
    "\n",
    "Key Points:\n",
    "    Vanishing Gradient Problem:\n",
    "        1. Derivates of loss w.r.t weights values for Sigmoid: Always between 0 and 0.25 and Tanh: Range from -1 to 1\n",
    "        2. In deep nural network, will multiply more gradientes, if few gradients are very small then result will be very small\n",
    "        3. While initializing weight, if we assign smaller weights then gradients will be very small\n",
    "        Whenever derivates of loss w.r.t weights will be very small then normally w_new = w_old - lr(small(dL/dw_old)) and learning rate mostly 0.01           then w_new ~= w_old. means no or very very small weight update happening so nuron learning nothing. This is called vanishing gradients\n",
    "\n",
    "    Non zero centered:\n",
    "        A non zero centered activation function is a function which output values are always positive.\n",
    "        Ex: tanh output ranges from -1 to 1, Leaky ReLu ranges from negative infinity to positive infinity\n",
    "        Gradients become biased in one direction\n",
    "        Weight updates become slower and less efficient\n",
    "        Training can be zig-zag and slow traing\n",
    "\n",
    "    zero centered:\n",
    "        A zero centered activation function is a function which output values are positive and negative.\n",
    "        Ex: Sigmoid output betwen 0 and 1, ReLu output range from 0 to positive infinite.\n",
    "        Gradients are balnaced\n",
    "        Weight updates become faster\n",
    "        Learning is faster\n",
    "\n",
    "    Dead Nueron:\n",
    "        Dead nueron is a nueron that never activates, means always activation function returns 0 output\n",
    "        Gradients become 0, weight updates stop, never learn anything and performance gets worse.\n",
    "\n",
    "    Vanishing Gradient vs Dead Nueron:\n",
    "        Vanishing gradient → gradients are very small. Weights barely move ⇒ learning very slow.\n",
    "        Dead neuron → gradient is exactly zero. Weights never move ⇒ neuron never learns again\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7aef82-a791-46a3-bd55-53bff9717939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b013c62-1baf-420d-8bbf-6b9fb10aa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output is bewtween 0 and 1 in sigmoid activatition function\n",
    "def sigmoid_activation_func(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e8ef7a-4754-472f-a0fd-4e8e07c52b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_activation_func(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1755fdff-3cbc-4e77-80bd-a1c5aed46ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_activation_func(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "612f721b-aa31-44b9-8892-7120797dabe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999546021312976"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_activation_func(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e503ffbb-4be4-444a-ab67-44a3c72f21f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5397868702434395e-05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_activation_func(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a5347f-66b0-42c4-a8c9-1d84b1793bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output is bewtween -1 and 1 in tanh activatition function\n",
    "def tanh_activation_function(x):\n",
    "    return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ec86067-5eed-4d25-83be-7561438689b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_activation_function(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8f7f04-b9bd-4968-b706-f364ec5bae6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7615941559557649"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_activation_function(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11f2b364-2d31-414f-9935-952b9ce91638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615941559557649"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_activation_function(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "278a70b4-7493-499e-8c0b-458aa43ca858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999958776926"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_activation_function(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a4c0266-da23-494e-a2f5-e02d4309b426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999958776926"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_activation_function(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c9eb205-391c-4d29-8598-343abb8d870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_activation_function(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c9da6a-37e7-4cf8-a488-ef8b1864499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Relu output for possitive values it should be positive but for negative values it becomes 0\n",
    "def ReLu_activation_function(x):\n",
    "    return max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d577d38a-61e5-4b90-9782-cb1b3cb853c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLu_activation_function(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ea8aa04-def8-4da0-a28d-bd6868040754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLu_activation_function(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "573d5cc3-d605-4d01-b1f5-32c8873cda49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLu_activation_function(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15a1b1f7-0600-4845-98d2-190a7286cf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLu_activation_function(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5511235b-ea10-4b8c-a2d7-58d2bf38da88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLu_activation_function(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e2cb564-1e2b-477b-a97a-216638171f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check leaky Relu output for possitive values it should be positive but for negative values it becomes small negative value\n",
    "def leaky_ReLu_activation_function(x):\n",
    "    a = 0.01\n",
    "    return max(a * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19feefad-a212-4a34-87f2-788e124cd6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_ReLu_activation_function(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bbeca01-cbeb-44db-9f0b-a75a24f31c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_ReLu_activation_function(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65e928e6-02a7-4552-9c6c-03b6ce7601b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_ReLu_activation_function(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eac03c69-15c1-47c7-97b5-0a985f078a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_ReLu_activation_function(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "558b8cb4-cc3c-4a1a-ac12-51358bfe7bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_ReLu_activation_function(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153c2d5-d0a3-4ba2-a7e4-8b0e01fba1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activate_funtion(x):\n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
